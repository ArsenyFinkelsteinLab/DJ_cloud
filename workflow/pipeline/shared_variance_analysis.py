"""This module was auto-generated by datajoint from an existing schema"""

import datajoint as dj
import numpy as np
import scipy
# import os.path
from bisect import bisect
import math
from math import *

schema = dj.Schema('lee_meso_analysis')

exp2 = dj.VirtualModule('exp2', 'arseny_s1alm_experiment2')
img = dj.VirtualModule('img', 'arseny_learning_imaging')
stimanal = dj.VirtualModule('stimanal', 'arseny_learning_photostim_anal')


def FetchChunked(relation, idx_name, val_name, chunk_size):
    idx = relation.fetch(idx_name, order_by=idx_name)
    num_elements = len(idx)
    num_chunks = (num_elements + (chunk_size - 1)) // chunk_size
    parts = []
    for i_chunk in range(num_chunks):
        i = i_chunk * chunk_size + 1
        # Don't need to manually check for the remainder; relation does it
        rel_part = relation & f"{idx_name} >= {i}" & f"{idx_name} < {i + chunk_size}"
        parts.append(np.asarray(rel_part.fetch(val_name, order_by=idx_name)))
    return np.concatenate(parts)


def InsertChunked(relation, data, chunk_size):
    num_elements = len(data)
    num_chunks = (num_elements + chunk_size - 1) // chunk_size
    for i_chunk in range(num_chunks):
        i = i_chunk * chunk_size
        relation.insert(data[i : min(i + chunk_size, num_elements)])


def MakeBins(F, bin_size):
    ceiled_bin_size = math.ceil(bin_size)
    if ceiled_bin_size == 0:
        return F
    num_bins = len(F) // ceiled_bin_size
    return [sum(F[i * ceiled_bin_size : (i + 1) * ceiled_bin_size]) / ceiled_bin_size for i in range(num_bins)]


def NormalizeF(F_binned, threshold, flag_zscore):
    if threshold > 0:
        F_zscored = scipy.stats.zscore(F_binned, 1)
        for i, fzs in enumerate(F_zscored):
            if fzs <= threshold:
                F_binned[i] = 0
    if flag_zscore: # zscoring the data
        return scipy.stats.zscore(F_binned, 1)
    else: # only centering the data
        return [f - fm for f, fm in zip(F_binned, np.mean(F_binned, 1))]


def FloatRange(start, stop, step):
    num_steps = int((stop - start) / step) + 1
    return [start + i * step for i in range(num_steps)]


@schema
class SVC(dj.Computed):
    definition = """
    -> exp2.SessionEpoch
    threshold_for_event  : double                       # threshold in zscore, after binning. 0 means we don't threshold. 1 means we take only positive events exceeding 1 std, 2 means 2 std etc.
    time_bin             : double                       # time window used for binning the data. 0 means no binning
    ---
    svc_components       : longblob                     # contribution of the temporal components to the SVC; fetching this for train/test neurons should give U in SVC of size (train/test neurons x components) for the top num_comp components

    set_indentity        : varchar(400)                 # possible values: {train, test}
    """

    @property
    def key_source(self):
        return (exp2.SessionEpoch & img.ROIdeltaF & stimanal.MiceIncluded) - exp2.SessionEpochSomatotopy

    def make(self, key):
    	# So far the code is only correct for threshold == 0
        thresholds_for_event = [0] # [0, 1, 2]

        rel_temp = img.Mesoscope & key
        if len(rel_temp) > 0:
            time_bin_vector = [0]   # Mesoscope session
        else:
            time_bin_vector = [0.2, 0.5, 1]

        flag_zscore = 0

        rel_data1 = (img.ROIdeltaF & key) - img.ROIBad
        self2 = SVCSharedVariance
        self3 = SVCTotalVariance
        for i, time_bin in enumerate(time_bin_vector):
            self.compute_SVC(self2, self3, key, rel_data1, flag_zscore, time_bin, thresholds_for_event)

    def compute_SVC(self, self2, self3, key, rel_data1, flag_zscore, time_bin, thresholds_for_event):
        rel_FOVEpoch = img.FOVEpoch & key
        rel_FOV = img.FOV & key
        if 'imaging_frame_rate' in rel_FOVEpoch.heading.secondary_attributes:
            imaging_frame_rate = rel_FOVEpoch.fetch1('imaging_frame_rate')
        else:
            imaging_frame_rate = rel_FOV.fetch1('imaging_frame_rate')

        # TODO: Use unique_roi_number or something else to guarantee consistent order
        # (but unique_roi_number is not a primary key)

        if 'dff_trace' in rel_data1.heading.secondary_attributes:
            F = FetchChunked(rel_data1 & key, 'roi_number', 'dff_trace', 500)
        else:
            F = FetchChunked(rel_data1 & key, 'roi_number', 'spikes_trace', 500)

        F_binned = np.array([MakeBins(Fi.flatten(), time_bin * imaging_frame_rate) for Fi in F])

        for threshold in thresholds_for_event:
            F_normalized = NormalizeF(F_binned, threshold, flag_zscore)
            
            npc = 1000
            n = F_normalized.shape[0]
            nhalf = np.floor(n/2)
            ntrain = list(range(1, nhalf))
            ntest = list(range(nhalf+1, nhalf*2))
            
            t = F_normalized.shape[1]
            thalf = np.floor(t/2)
            itrain = list(range(1, thalf))
            itest = list(range(thalf+1, thalf*2))
                
            sneur, varneur, u, v = svca(F_normalized, npc, ntrain, ntest, itrain, itest)

            # Populating MESO.SVCSharedVariance and MESO.SVCTotalVariance
            svc_key = {**key, 'time_bin': time_bin, 'threshold_for_event': threshold}
            self2.insert1({**svc_key, 'shared_variance': s}, allow_direct_insert=True)
            self3.insert1({**svc_key, 'total_variance': s}, allow_direct_insert=True)
            
            
    def svca(F, npc, ntrain, ntest, itrain, itest)
        cov = F[ntrain, itrain] * np.transpose(F[ntest, itrain])
        u, s, vh = np.linalg.svd(cov, full_matrices=False)
        u = u[, 1:npc]
        v = v[, 1:npc]
        s1 = np.transpose(u) * F[ntrain, itest]
        s2 = vh * F[ntest, itest]
        sneur = np.sum(s1 .* s2, dim=2)
        varneur = np.sum(s1**2 + s2**2, dim=2) / 2
        return [s1, s2, sneur, varneur]


@schema
class SVCSharedVariance(dj.Computed):
    definition = """
    -> exp2.SessionEpoch
    threshold_for_event  : double                       # threshold in deltaf_overf
    time_bin             : double                       # time window used for binning the data. 0 means no binning
    ---
    shared_variance      : longblob                     # shared variance across SVCs, computed a la Stringer
    """

@schema
class SVCTotalVariance(dj.Computed):
    definition = """
    -> exp2.SessionEpoch
    threshold_for_event  : double                       # threshold in deltaf_overf
    time_bin             : double                       # time window used for binning the data. 0 means no binning
    ---
    total_variance      : longblob                     # total variance of activity along SVCs
    """

