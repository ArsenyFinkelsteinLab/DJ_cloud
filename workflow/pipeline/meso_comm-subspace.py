"""This module was auto-generated by datajoint from an existing schema"""

import datajoint as dj
import numpy as np
import scipy
from scipy import sparse
import math
from math import *

schema = dj.Schema('lee_meso_analysis')

exp2 = dj.VirtualModule('exp2', 'arseny_s1alm_experiment2')
img = dj.VirtualModule('img', 'arseny_learning_imaging')
stimanal = dj.VirtualModule('stimanal', 'arseny_learning_photostim_anal')
lab = dj.VirtualModule('lab', 'map_lab')


def FetchChunked(relation_area, relation_tot, idx_name, val_name, chunk_size):
    idx = relation_tot.fetch(idx_name, order_by=idx_name)
    num_elements = len(idx)
    num_chunks = (num_elements + (chunk_size - 1)) // chunk_size
    parts = []
    for i_chunk in range(num_chunks):
        i = i_chunk * chunk_size + 1
        # Don't need to manually check for the remainder; relation does it
        rel_part = relation_area & f"{idx_name} >= {i}" & f"{idx_name} < {i + chunk_size}"
        parts.append(np.asarray(rel_part.fetch(val_name, order_by=idx_name)))
    return np.concatenate(parts)


def MakeBins(F, bin_size):
    ceiled_bin_size = math.ceil(bin_size)
    if ceiled_bin_size == 0:
        return F
    num_bins = len(F) // ceiled_bin_size
    return [sum(F[i * ceiled_bin_size : (i + 1) * ceiled_bin_size]) / ceiled_bin_size for i in range(num_bins)]


def NormalizeF(F_binned, threshold, flag_zscore):
    if threshold > 0:
        F_zscored = scipy.stats.zscore(F_binned, 1)
        for i, fzs in enumerate(F_zscored):
            if fzs <= threshold:
                F_binned[i] = 0
    if flag_zscore: # zscoring the data
        return scipy.stats.zscore(F_binned, 1)
    else: # only centering the data
        return [f - fm for f, fm in zip(F_binned, np.mean(F_binned, 1))]


def FloatRange(start, stop, step):
    num_steps = int((stop - start) / step) + 1
    return [start + i * step for i in range(num_steps)]


@schema
class CommSubspace(dj.Computed):
    definition = """
    -> exp2.SessionEpoch
    -> meso.BrainAreaPairs
    threshold_for_event  : double                       # threshold in deltaf_overf
    time_bin             : double                       # time window used for binning the data. 0 means no binning
    ---
    rank_values          : blob
    target_r2   : longblob
    source_r2   : longblob
    """

    @property
    def key_source(self):
        return (exp2.SessionEpoch*lab.BrainArea & img.ROIdeltaF & img.ROIBrainArea & 'brain_area = "MOp') - exp2.SessionEpochSomatotopy

    def make(self, key):
    	# So far the code is only correct for threshold == 0
        thresholds_for_event = [0] # [0, 1, 2]

        brain_area_list = ["MOs", "RSPagl", "RSPd", "SSp-bfd", "SSp-tr", "SSp-ul", "SSp-un"]
        rel_temp = img.Mesoscope & key
        time_bin = [0]

        flag_zscore = 1
        sigma = .1

        rel_FOVEpoch = img.FOVEpoch & key
        rel_FOV = img.FOV & key
        rel_data_area = (img.ROIdeltaF*img.ROIBrainArea & key) - img.ROIBad
        rel_data_tot = (img.ROIdeltaF & key) - img.ROIBad

        if 'imaging_frame_rate' in rel_FOVEpoch.heading.secondary_attributes:
            imaging_frame_rate = rel_FOVEpoch.fetch1('imaging_frame_rate')
        else:
            imaging_frame_rate = rel_FOV.fetch1('imaging_frame_rate')

        F = FetchChunked(rel_data_area & key, rel_data_tot & key, 'roi_number', 'dff_trace', 500)



        for area2 in brain_area_list:

            key2 = key
            key2['brain_area'] = area2

            F_binned = np.array([MakeBins(Fi.flatten(), time_bin * imaging_frame_rate) for Fi in F])
            nneurons = F_binned.shape[0]
            ntimepoints = F_binned.shape[1]
            nneurons = min(nneurons,2000)

            F2 = FetchChunked(rel_data_area & key2, rel_data_tot & key2, 'roi_number', 'dff_trace', 500)

            F_binned2 = np.array([MakeBins(Fi.flatten(), time_bin * imaging_frame_rate) for Fi in F2])
            nneurons2 = F_binned2.shape[0]
            nneurons = min(nneurons,nneurons2)

            F1 = F[:nneurons,:]
            F2 = F2[:nneurons,:]

            rank_values = range(0,21)
            rank_values = [rank_values, range(30,101,10), range(150,nneurons,50)]

            for rank in rank_values:
                mse, ss = reduced_reg(F1,F2,rank,sigma)




            def reduced_reg(X,Y,rank,sigma):
                mX = np.mean(X,2)
                mY = np.mean(Y,2)
                X = X - mX
                Y = Y - mY

                CXX = np.linalg.pinv(np.dot(X.T,X))
                B_OLS = np.dot(CXX + sigma * sparse.eye(np.size(X,2))), np.dot(X.T,Y)
                
                Y_OLS = np.dot(X,B_OLS)
                _U, _S, V = np.linalg.svd(Y_OLS)
                Vr = V[:,:rank]
                
                B = B_OLS
                if rank > 0:
                    B = np.dot(B, np.dot(Vr,Vr.T))

                err = Y - np.dot(X,B)
                err = err.flatten
                mse = np.mean(np.power(err,2))
                Y = Y.flatten()
                ss = np.mean(np.power(Y,2))

           

            # key_ROIs = (rel_data_area & key).fetch('KEY', order_by='roi_number')
            # for i in range(500):
            #     key_ROIs[i]['roi_components'] = u_limited[i]
            #     key_ROIs[i]['time_bin'] = time_bin
            #     key_ROIs[i]['threshold_for_event'] = threshold

            # InsertChunked(self, key_ROIs, 1000)

            # svd_key = {**key, 'time_bin': time_bin, 'threshold_for_event': threshold}
            # self2.insert1({**svd_key, 'singular_values': s}, allow_direct_insert=True)
            # key_temporal = [{**svd_key, 'component_id': ic, 'temporal_component': vt[ic]}
            #                 for ic in range(num_components_save)]
            # self3.insert(key_temporal, allow_direct_insert=True)


@schema
class SVDAreaSingularValues2(dj.Computed):
    definition = """
    -> exp2.SessionEpoch
    -> lab.BrainArea
    threshold_for_event  : double                       # threshold in deltaf_overf
    time_bin             : double                       # time window used for binning the data. 0 means no binning
    ---
    singular_values      : longblob                     # singular values of each SVD temporal component, ordered from larges to smallest value
    """


@schema
class SVDAreaTemporalComponents2(dj.Computed):
    definition = """
    -> exp2.SessionEpoch
    -> lab.BrainArea
    component_id         : int                          
    threshold_for_event  : double                       # threshold in deltaf_overf
    time_bin             : double                       # time window used for binning the data. 0 means no binning
    ---
    temporal_component   : longblob                     # temporal component after SVD (fetching this table for all components should give the Vtransopose matrix from SVD) of size (components x frames). Includes the top num_comp components
    """
